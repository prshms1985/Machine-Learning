{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prashamsh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:99: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from urllib import request\n",
    "\n",
    "url1 = \"https://www.gutenberg.org/files/34842/34842-0.txt\" # could not find this in 'fileids' in NLTK library, so I am providing the absolute path\n",
    "response1 = request.urlopen(url1)\n",
    "document = response1.read().decode(\"utf-8-sig\")\n",
    "\n",
    "#document = \"be called the illustrated _vers de société_ of the Louis 90. Certainly Chicago up to the time of Géricault painting in general held. Citing high fuel prices, John is a great guy, United airlines on friday mentioned United expects to offset about 90 percent of the increase in fuel prices this year 2009. Travelers are willing to pay up for more expensive seats, including those with more legroom and other perks, executives said on a call with analysts on Wednesday. The results quieted worries earlier in the year that United’s aggressive growth plan would lead to a fare war with other carriers\"\n",
    "sentences = nltk.sent_tokenize(document)\n",
    "sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "#print (sentences)\n",
    "\n",
    "arr = []\n",
    "\n",
    "def RepresentsInt(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# Preparing the data to be fed into the algorithm. Capturing word, word's POS tag, word length, next word's pos, whether it is NE or not\"\n",
    "# Also included special condition to include numbers in the NER as the system was not catching them\n",
    "for sentence in sentences:\n",
    "    count = 0\n",
    "    for i in nltk.ne_chunk(sentence):\n",
    "        if count+1 == len(nltk.ne_chunk(sentence)):\n",
    "            continue\n",
    "        else:\n",
    "            j = nltk.ne_chunk(sentence)[count+1]\n",
    "        count = count+1\n",
    "\n",
    "        if isinstance(i, tuple):\n",
    "            if RepresentsInt(i[0]) and isinstance(j, tuple):\n",
    "                arr.append([i[0],i[1],len(i[0]),j[1],'Y'])\n",
    "            elif RepresentsInt(i[0]) and not isinstance(j, tuple):\n",
    "                arr.append([i[0],i[1],len(i[0]),j[0][1],'Y'])                                \n",
    "            elif isinstance(j, tuple) and not RepresentsInt(i[0]):\n",
    "                    arr.append([i[0],i[1],len(i[0]),j[1],'N'])\n",
    "            elif not isinstance(j, tuple) and not RepresentsInt(i[0]):\n",
    "                    arr.append([i[0],i[1],len(i[0]),j[0][1],'N'])\n",
    "        else:\n",
    "            if isinstance(j, tuple):\n",
    "                    arr.append([i[0][0],i[0][1],len(i[0][0]),j[1],'Y'])\n",
    "            else:\n",
    "                    arr.append([i[0][0],i[0][1],len(i[0][0]),j[0][1],'Y'])\n",
    "\n",
    "#print (arr[0][3])\n",
    "\n",
    "df = pd.DataFrame(arr, columns=['word', 'word_pos', 'word_len', 'next_word_pos', 'ner'])\n",
    "\n",
    "word_Map = {}\n",
    "\n",
    "for i, j in enumerate(np.unique(df['word'])):\n",
    "    word_Map[j] = i\n",
    "\n",
    "word_pos_Map = {}\n",
    "\n",
    "for i, j in enumerate(np.unique(df['word_pos'])):\n",
    "    word_pos_Map[j] = i\n",
    "\n",
    "next_word_pos_Map = {}\n",
    "for i, j in enumerate(np.unique(df['next_word_pos'])):\n",
    "    next_word_pos_Map[j] = i\n",
    "\n",
    "class_mapping = {label: idx for idx, label in enumerate(np.unique(df['word']))}\n",
    "class_mapping\n",
    "df['word'] = df['word'].map(class_mapping)\n",
    "df\n",
    "\n",
    "class_mapping = {label: idx for idx, label in enumerate(np.unique(df['word_pos']))}\n",
    "class_mapping\n",
    "df['word_pos'] = df['word_pos'].map(class_mapping)\n",
    "df\n",
    "\n",
    "class_mapping = {label: idx for idx, label in enumerate(np.unique(df['next_word_pos']))}\n",
    "class_mapping\n",
    "df['next_word_pos'] = df['next_word_pos'].map(class_mapping)\n",
    "df\n",
    "\n",
    "class_mapping = {label: idx for idx, label in enumerate(np.unique(df['ner']))}\n",
    "class_mapping\n",
    "df['ner'] = df['ner'].map(class_mapping)\n",
    "df\n",
    "\n",
    "\n",
    " \n",
    "col_names = ['word', 'word_pos', 'word_len', 'next_word_pos']   \n",
    "X = df[col_names].values # X values\n",
    "y = pd.get_dummies(df['ner']).as_matrix() # y which is predictor variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 25)                125       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 52        \n",
      "=================================================================\n",
      "Total params: 177\n",
      "Trainable params: 177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Using Neural Networks\n",
    "\n",
    "NN_model = Sequential()\n",
    "NN_model.add(Dense(25, input_dim = 4, activation='relu'))\n",
    "NN_model.add(Dense(2, activation='softmax'))\n",
    "NN_model.compile(loss='categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "NN_model.summary()\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=20, verbose=0, mode='auto')\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='best_weights.hdf5',verbose=0, save_best_only=True)\n",
    "\n",
    "history = NN_model.fit(X_train, y_train, validation_data = (X_test,y_test), batch_size = 128, callbacks=[monitor, checkpointer], verbose=0, epochs=10)\n",
    "\n",
    "\n",
    "NN_model.load_weights('best_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion_matrix, without normalization\n",
      "[[  91  124]\n",
      " [  71 3055]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NER       0.56      0.42      0.48       215\n",
      "     Not NER       0.96      0.98      0.97      3126\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      3341\n",
      "   macro avg       0.76      0.70      0.73      3341\n",
      "weighted avg       0.94      0.94      0.94      3341\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import confusion_matrix\n",
    "pred=NN_model.predict(X_test)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "y_test2 = np.argmax(y_test, axis=1)\n",
    "\n",
    "y_test2 = np.where(y_test2==0, 'Not NER', y_test2)\n",
    "y_test2 = np.where(y_test2=='1', 'NER', y_test2)\n",
    "\n",
    "pred = np.where(pred==0, 'Not NER', pred)\n",
    "pred = np.where(pred=='1', 'NER', pred)\n",
    "\n",
    "cm = confusion_matrix(y_test2, pred)\n",
    "np.set_printoptions(precision=2)\n",
    "print ('Confusion_matrix, without normalization')\n",
    "print (cm)\n",
    "#plt.figure()\n",
    "print (classification_report(y_test2,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horsemen NER\n",
      "-------------------\n",
      "from Not NER\n",
      "-------------------\n",
      "the Not NER\n",
      "-------------------\n",
      "Parthenon NER\n",
      "-------------------\n",
      "Frieze._ NER\n",
      "-------------------\n",
      "The Not NER\n",
      "-------------------\n",
      "frieze Not NER\n",
      "-------------------\n",
      "of Not NER\n",
      "-------------------\n",
      "the Not NER\n",
      "-------------------\n",
      "Parthenon NER\n",
      "-------------------\n",
      "is Not NER\n",
      "-------------------\n",
      "part Not NER\n",
      "-------------------\n",
      "of Not NER\n",
      "-------------------\n",
      "the Not NER\n",
      "-------------------\n",
      "decorative Not NER\n",
      "-------------------\n",
      "scheme Not NER\n",
      "-------------------\n",
      "of Not NER\n",
      "-------------------\n",
      "the Not NER\n",
      "-------------------\n",
      "marble Not NER\n",
      "-------------------\n",
      "temple Not NER\n",
      "-------------------\n",
      "of Not NER\n",
      "-------------------\n",
      "Athena Not NER\n",
      "-------------------\n",
      ", Not NER\n",
      "-------------------\n",
      "built Not NER\n",
      "-------------------\n",
      "during Not NER\n",
      "-------------------\n",
      "the Not NER\n",
      "-------------------\n",
      "age Not NER\n",
      "-------------------\n",
      "of Not NER\n",
      "-------------------\n",
      "Pericles Not NER\n",
      "-------------------\n",
      "( Not NER\n",
      "-------------------\n",
      "480-430 NER\n",
      "-------------------\n",
      "B. Not NER\n",
      "-------------------\n",
      "C. Not NER\n",
      "-------------------\n",
      ") Not NER\n",
      "-------------------\n",
      "on Not NER\n",
      "-------------------\n",
      "the Not NER\n",
      "-------------------\n",
      "Acropolis NER\n",
      "-------------------\n",
      ", Not NER\n",
      "-------------------\n",
      "Athens Not NER\n",
      "-------------------\n",
      ", Not NER\n",
      "-------------------\n",
      "and Not NER\n",
      "-------------------\n",
      "decorated Not NER\n",
      "-------------------\n",
      "under Not NER\n",
      "-------------------\n",
      "the Not NER\n",
      "-------------------\n",
      "direction Not NER\n",
      "-------------------\n",
      "of Not NER\n",
      "-------------------\n",
      "Phidias Not NER\n",
      "-------------------\n",
      "The Not NER\n",
      "-------------------\n",
      "frieze Not NER\n",
      "-------------------\n",
      "consisted Not NER\n",
      "-------------------\n",
      "of Not NER\n",
      "-------------------\n",
      "a Not NER\n",
      "-------------------\n",
      "series Not NER\n",
      "-------------------\n",
      "of Not NER\n",
      "-------------------\n",
      "panels Not NER\n",
      "-------------------\n",
      "or Not NER\n",
      "-------------------\n",
      "slabs Not NER\n",
      "-------------------\n",
      ", Not NER\n",
      "-------------------\n",
      "about Not NER\n",
      "-------------------\n",
      "3 NER\n",
      "-------------------\n",
      "ft. Not NER\n",
      "-------------------\n",
      "4 Not NER\n",
      "-------------------\n",
      "in Not NER\n",
      "-------------------\n",
      "in Not NER\n",
      "-------------------\n",
      "height Not NER\n",
      "-------------------\n",
      ", Not NER\n",
      "-------------------\n",
      "and Not NER\n",
      "-------------------\n",
      "was Not NER\n",
      "-------------------\n",
      "set Not NER\n",
      "-------------------\n",
      "on Not NER\n",
      "-------------------\n",
      "the Not NER\n",
      "-------------------\n",
      "outer Not NER\n",
      "-------------------\n",
      "wall Not NER\n",
      "-------------------\n",
      "of Not NER\n",
      "-------------------\n",
      "the Not NER\n",
      "-------------------\n",
      "cella Not NER\n",
      "-------------------\n",
      "Being Not NER\n",
      "-------------------\n",
      "lighted Not NER\n",
      "-------------------\n",
      "from Not NER\n",
      "-------------------\n",
      "below Not NER\n",
      "-------------------\n",
      ", Not NER\n",
      "-------------------\n",
      "the Not NER\n",
      "-------------------\n",
      "lower Not NER\n",
      "-------------------\n",
      "portion Not NER\n",
      "-------------------\n",
      "is Not NER\n",
      "-------------------\n",
      "cut Not NER\n",
      "-------------------\n",
      "in Not NER\n",
      "-------------------\n",
      "low Not NER\n",
      "-------------------\n",
      "relief Not NER\n",
      "-------------------\n",
      "( Not NER\n",
      "-------------------\n",
      "1¼ NER\n",
      "-------------------\n",
      "in Not NER\n",
      "-------------------\n",
      ". Not NER\n",
      "-------------------\n",
      "and Not NER\n",
      "-------------------\n",
      "the Not NER\n",
      "-------------------\n",
      "upper Not NER\n",
      "-------------------\n",
      "parts Not NER\n",
      "-------------------\n",
      "in Not NER\n",
      "-------------------\n",
      "high Not NER\n",
      "-------------------\n",
      "relief Not NER\n",
      "-------------------\n",
      "Iliad Not NER\n",
      "-------------------\n",
      ", Not NER\n",
      "-------------------\n",
      "Book Not NER\n",
      "-------------------\n",
      "v. Not NER\n",
      "-------------------\n",
      ", Not NER\n",
      "-------------------\n",
      "lines Not NER\n",
      "-------------------\n",
      "309 NER\n",
      "-------------------\n",
      "and Not NER\n",
      "-------------------\n",
      "352 NER\n",
      "-------------------\n",
      "and Not NER\n",
      "-------------------\n",
      "Certainly NER\n",
      "-------------------\n",
      "up Not NER\n",
      "-------------------\n",
      "to Not NER\n",
      "-------------------\n",
      "the Not NER\n",
      "-------------------\n",
      "time Not NER\n",
      "-------------------\n",
      "of Not NER\n",
      "-------------------\n",
      "Géricault Not NER\n",
      "-------------------\n",
      "painting Not NER\n",
      "-------------------\n",
      "in Not NER\n",
      "-------------------\n",
      "general Not NER\n",
      "-------------------\n",
      "held Not NER\n",
      "-------------------\n",
      "itself Not NER\n",
      "-------------------\n",
      "rather Not NER\n",
      "-------------------\n",
      "pedantically Not NER\n",
      "-------------------\n",
      "aloof Not NER\n",
      "-------------------\n",
      "from Not NER\n",
      "-------------------\n",
      "poetry Not NER\n",
      "-------------------\n",
      "Claude Not NER\n",
      "-------------------\n",
      ", Not NER\n",
      "-------------------\n",
      "Chardin Not NER\n",
      "-------------------\n",
      ", Not NER\n",
      "-------------------\n",
      "what Not NER\n",
      "-------------------\n",
      "may Not NER\n",
      "-------------------\n",
      "be Not NER\n",
      "-------------------\n",
      "called Not NER\n",
      "-------------------\n",
      "the Not NER\n",
      "-------------------\n",
      "illustrated Not NER\n",
      "-------------------\n",
      "_vers Not NER\n",
      "-------------------\n",
      "de Not NER\n",
      "-------------------\n",
      "société_ Not NER\n",
      "-------------------\n",
      "of Not NER\n",
      "-------------------\n",
      "the Not NER\n",
      "-------------------\n",
      "Louis Not NER\n",
      "-------------------\n",
      "painters Not NER\n",
      "-------------------\n",
      "-- Not NER\n",
      "-------------------\n",
      "of Not NER\n",
      "-------------------\n",
      "Watteau Not NER\n",
      "-------------------\n",
      "and Not NER\n",
      "-------------------\n",
      "Fragonard Not NER\n",
      "-------------------\n",
      "-- Not NER\n",
      "-------------------\n",
      "even Not NER\n",
      "-------------------\n",
      "Prudhon Not NER\n",
      "-------------------\n",
      ", Not NER\n",
      "-------------------\n",
      "did Not NER\n",
      "-------------------\n",
      "little Not NER\n",
      "-------------------\n",
      "to Not NER\n",
      "-------------------\n",
      "change Not NER\n",
      "-------------------\n",
      "the Not NER\n",
      "-------------------\n",
      "prevailing Not NER\n",
      "-------------------\n",
      "color Not NER\n",
      "-------------------\n",
      "and Not NER\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "# Code where new text can be tested\n",
    "\n",
    "import operator\n",
    "new_text = 'Horsemen from the Parthenon Frieze._ The frieze of the Parthenon is part of the decorative scheme of the marble temple of Athena, built during the age of Pericles (480-430 B. C.) on the Acropolis, Athens, and decorated under the direction of Phidias. The frieze consisted of a series of panels or slabs, about 3 ft. 4 in. in height, and was set on the outer wall of the cella. Being lighted from below, the lower portion is cut in low relief (1¼ in.) and the upper parts in high relief. Iliad, Book v., lines 309 and 352 and Certainly up to the time of Géricault painting in general held itself rather pedantically aloof from poetry. Claude, Chardin, what may be called the illustrated _vers de société_ of the Louis Quinze painters--of Watteau and Fragonard--even Prudhon, did little to change the prevailing color and tone'\n",
    "\n",
    "test_sentences = nltk.sent_tokenize(new_text)\n",
    "test_sentences = [nltk.word_tokenize(sent) for sent in test_sentences]\n",
    "test_sentences = [nltk.pos_tag(sent) for sent in test_sentences]\n",
    "\n",
    "test_arr = []\n",
    "for sentence in test_sentences:\n",
    "    count = 0\n",
    "    for i in nltk.ne_chunk(sentence):\n",
    "        if count+1 == len(nltk.ne_chunk(sentence)):\n",
    "            continue\n",
    "        else:\n",
    "            j = nltk.ne_chunk(sentence)[count+1]\n",
    "        count = count+1\n",
    "\n",
    "        if isinstance(i, tuple):\n",
    "            if isinstance(j, tuple):\n",
    "                test_arr.append([i[0],i[1],len(i[0]),j[1]])\n",
    "            elif not isinstance(j, tuple):\n",
    "                test_arr.append([i[0],i[1],len(i[0]),j[0][1]])                                \n",
    "        else:\n",
    "            if isinstance(j, tuple):\n",
    "                    test_arr.append([i[0][0],i[0][1],len(i[0][0]),j[1]])\n",
    "            else:\n",
    "                    test_arr.append([i[0][0],i[0][1],len(i[0][0]),j[0][1]])\n",
    "                    \n",
    "df1 = pd.DataFrame(test_arr, columns=['word', 'word_pos', 'word_len', 'next_word_pos'])\n",
    "df2 = pd.DataFrame(test_arr, columns=['word', 'word_pos', 'word_len', 'next_word_pos'])\n",
    "col_names_test = ['word', 'word_pos', 'word_len', 'next_word_pos']\n",
    "\n",
    "for i in range(len(df1)):\n",
    "    count = 0\n",
    "    if df1.iloc[i,0] in word_Map:\n",
    "        df1.iloc[i,0] = word_Map[df1.iloc[i,0]]\n",
    "    else:\n",
    "        count = max(word_Map.items(), key=operator.itemgetter(1))[1]+1\n",
    "        df1.iloc[i,0] = count\n",
    "    if df1.iloc[i,1] in word_pos_Map:\n",
    "        df1.iloc[i,1] = word_pos_Map[df1.iloc[i,1]]\n",
    "    else:\n",
    "        count = max(word_pos_Map.items(), key=operator.itemgetter(1))[1]+1\n",
    "        df1.iloc[i,1] = count\n",
    "    if df1.iloc[i,3] in next_word_pos_Map:\n",
    "        df1.iloc[i,3] = next_word_pos_Map[df1.iloc[i,3]]\n",
    "    else:\n",
    "        count = max(next_word_pos_Map.items(), key=operator.itemgetter(1))[1]+1\n",
    "        df1.iloc[i,3] = count\n",
    "\n",
    "X1 = df1[col_names_test].values\n",
    "pred_test=NN_model.predict(X1)\n",
    "pred_test = np.argmax(pred_test, axis=1)\n",
    "\n",
    "\n",
    "pred_test = np.where(pred_test==0, 'Not NER', pred_test)\n",
    "pred_test = np.where(pred_test=='1', 'NER', pred_test)\n",
    "\n",
    "for i in range(len(df1)):\n",
    "    print (df2.iloc[i,0], pred_test[i])\n",
    "    print (\"-------------------\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
