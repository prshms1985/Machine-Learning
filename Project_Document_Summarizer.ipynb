{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Summary: \n",
      "\n",
      "[9] Due to the collapse of the 4-story Chuzon Supermarket, the Department of Interior and Local Government had to suspend all business permits of Chuzon Supermarket and its branches, as well as to conduct an investigation regarding the collapse of the 4-storey commercial establishment, which was built 4 years ago.. [13] PHIVOLCS added that the earthquake would not trigger an eruption of Pinatubo, stating that the volcano's magma supply has not sufficiently replenished since 1991 to allow for another eruption.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def generate_sentences(filename):\n",
    "\n",
    "    file = open(filename, encoding=\"utf8\")\n",
    "    rawtext = file.read()\n",
    "    sentences = []\n",
    "    # tokenizing the sentences\n",
    "    token_sent = nltk.sent_tokenize(rawtext)\n",
    "\n",
    "    for sentence in token_sent:\n",
    "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
    "    return sentences\n",
    "    \n",
    "def generate_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    "    #Build empty vectors with columns equal to lenght of all words\n",
    "    vector1 = [0] * len(all_words) # Value 0 works perfectly for this, any value other than 0 gives different results\n",
    "    vector2 = [0] * len(all_words)\n",
    "\n",
    "    #building vector for the first sentence by adding values\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1 # value can be increased by any count, keeping it similar to vector2 increment gives good results\n",
    " \n",
    "        #building vector for the first sentence by adding values\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    "    # This returns cosine distance\n",
    "    return cosine_distance(vector1, vector2)\n",
    " \n",
    "def generate_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix to be updated below with values\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: \n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = generate_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "    #returning the generated similarity matrix\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def generate_summarized_text(file_name, num_sent=3):\n",
    "    stop_words = stopwords.words('english')\n",
    "    summarize_text = []\n",
    "\n",
    "    # Gathering the sentences split into array of words in a multi dimensional array\n",
    "    sentences =  generate_sentences(file_name)\n",
    "\n",
    "    # generating similarity matrix for each set of sentence combinations\n",
    "    generate_similarity_martix = generate_similarity_matrix(sentences, stop_words)\n",
    "\n",
    "    # ranking the sentence in the similarity matrix\n",
    "    generate_similarity_graph = nx.from_numpy_array(generate_similarity_martix)\n",
    "    generate_scores = nx.pagerank(generate_similarity_graph)\n",
    "\n",
    "    # Sorting the sentences in terms of rank and picking the top sentences\n",
    "    generate_ranked_sentence = sorted(((generate_scores[i],s) for i,s in enumerate(sentences)), reverse=False)     \n",
    "\n",
    "    for i in range(num_sent):\n",
    "      summarize_text.append(\" \".join(generate_ranked_sentence[i][1]))\n",
    "\n",
    "    #Printing the summary\n",
    "    print(\"Document Summary: \\n\")\n",
    "    print (\". \".join(summarize_text))\n",
    "\n",
    "# This is where we provide the file name that needs to be uploaded from our machine   \n",
    "generate_summarized_text('test_sen_len.txt', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
